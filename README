=======================================
Optimizations of the Common Convolution
Samuel L Owens
==============
This collection shows various stages along the optimization process using convolutions as an example

Step 1: Basic Sequential Implementation : sequential_basic.cpp
	this is the first functional build 
	no attention is paid to optimizations
	simply proves the solution is valid 
	:
	my solution is very basic, it should be understandable to most anyone familiar with c/c++

Step 2: Optimized Sequential Implementation : sequential_opt.cpp
	this is the first round of optimizations
	remove any redundant instructions 
	minimize variable initializations
	merge for loops to minimize iteration instructions
	target edge cases with optimized solution variants
	this might look longer and more complicated but the output should be identical
	:
	my solution becomes more complex
	each edge case is given its own targeted function
		this removes redundant if statements
	for loops are collapsed where one of their dimensions was the same
		this removes redundant iteration instructions
	variables initializations are minimized 


Step 3: Basic Multithreaded : multi_basic.cpp
	this is the first attempt at a multithreaded solution
	if possible take your sequential solution and parallelize it
	openmp allows simple generic parallelizations without much effort
	carve your solution into steps that dont rely on one another 
	do each of those asynchronously with respect to the others
	sometimes you will need to rework your solution to be more parallel
	some things simply cannot be parallelized efficiently
	:
	my solution basically takes the optimized sequential solution and does it asynchronously
	this is a good example of OMP's ease of use,
	create a new task for each step of the solution that can be done asynchronously
	the core is treated as a single chunk


Step 4: Optimized Multithreaded : multi_opt_1.cpp
	this is the first round of optimizations to the multithreaded solution
	perhaps you've simply plugged in an omp instruction to your sequential with tweaking
	perhaps you've come to a new solution with more parallelization
	first repeat/revisit the optimizations of Step 2, you may have created some new redundancies
		it is possible some of your optimizations may be detrimental to parallelization
	now decide how large of chunks you should be working on,
		perhaps you should carve your work up further
		perhaps there are some memory access patterns in your solution
			optimizing to those patterns may yield significant speed up
		perhaps some things you are parallelizing are insignificant
			starting a new thread takes time and that is time not spent on your computations
	:
	my solution optimizes upon the previously multithreaded solution
	the core is the best remaining avenue for increased parallelism
	carving the core into chunks is the best way to do this 
	alternating threads in a cell by cell nature 
		where a cell from thread0 is directly next to a cell from thread1
		you will lose benefits from cachelines being larger than cell values in terms of bytes
		larger cachelines means each of the cores will read in a contiguous piece of memory
		because of how arrays are typically allocated the values are going to be continuous
		so each core is going to have multiple adjacent values in it's cache at a time
	so we parallelize by chunks not by cells
	it is recommended to carve the array vertically 
		( and horizontally also only if a single row is larger than the cacheline on your system )
	I have simply used OMP to carve it vertically, each thread will operate on 32 rows
	this lines up reasonably well with the cache sizes on my cpu
	I achieve aproximately ( N_CORESx ) speedup when allowing N_CORES cpu threads 

		
Step 5.1: CPU Architecture Targeting
	this is the final step in optimizations on the CPU side
	rework your solution to take advantage of AVX, SSE, SIMD, etc
		find what instruction sets are available on your architecture
		take advantage of the ones that might be helpful to you
	sometimes this can be a simple process 
	sometimes this can mean completely reworking your solution
	this almost always gives significant performance improvements
	using compiler optimizations can work well, often better than handwritten optimizations
		g++/gcc : -O1, -O2, -O3 for increasing levels of optimization
		be sure to verify the compiler does not optimize away things it believes are redundant
		it will delete parts of your solution if it thinks they dont matter
		it is more aggressive about this as you increase the level, 3 is the most aggressive

Step 5.2: GPU Acceleration
	some problems benefit from gpu acceleration 
	for these problems CUDA is a tremendous tool if you have an NVIDIA gpu
	OPENCL and MPI are reasonable alternatives if you do not have an NVIDIA gpu
	I will be using CUDA in this collection
	
	the gpu implementations follow a somewhat counter intuitive pattern as far as speedup is concerned
	in the CPU based implementations the more specific code with a specific function for each edge case is more
	efficient because the time needed to check every individual cell's index for validity is more than the time needed
	to initialize all of the threads needed, so doing a little extra work at the beginning to set up the threads pays off
	
	on the GPU only one CUDA kernel can be running at a time, but inside that kernel every thread in every block will be running simultaneously
	because every thread is doing the exact same thing on data that may have different values but is structurally the same the kernel 
	could be expected to finish running at the same speed regardless of the number of threads.

	Because adding more threads to a kernel won't make that kernel slower and only one kernel can run at a time, we want to pack as much work
	into each of our kernels as we can. And because we can expect the kernel to finish in approximately the amount of time needed to complete 
	one of its threads, we can assume the if statement would add only the time needed to pass through it once rather than M*N times.
	Only considering the time of one iteration of that if statement adds much less time than the time needed to initialize all of the individual kernels
	for the more specific implementation.
	So the basic implementation on the gpu is actually more efficient than the more complicated one because of the nature of how CUDA works.

Step 5.2.1: gpu_basic.cu
	this is the original sequential_basic implementation ported to run the GPU
	the convolution function is ported to a CUDA kernel
	we will use the amount of iteration limits from the for loops to specify the number of blocks and threads
	each row will be a block, each column will be a thread
	because every CUDA thread will run the same code block we use the thread and block index to find the cell index
	we use the same 4 part if statement to check the validity of the indices for each convolution
	we use the same multiplication scheme to acquire the new cell value

Step 5.2.2 gpu_opt.cu
	this is the optimized sequential implementation ported to run on the GPU
	each of the individual convolution functions are ported to their own CUDA kernel
	we will use the amount of iterations from the for loop to specify number of blocks and threads again
	again each row is a block and each column is a thread
	each of the CUDA kernels has a different offset applied to the index because the chunks are located in different places in the array
	we use the same multiplication scheme from the individual convolution functions in their new CUDA kernels



